---
title: Run the model on the edge device
weight: 6

### FIXED, DO NOT MODIFY
layout: learningpathall
---
Upon completing the final section, you will have put the model to the test by experimenting with different prompts on the embedded device.

## Set up your Raspberry Pi 5

If you want to see how the LLM behaves in an embedded environment, you need a Raspberry Pi 5 running Raspberry Pi OS. Install Raspberry Pi OS using the [Raspberry Pi documentation](https://www.raspberrypi.com/documentation/computers/getting-started.html). There are numerous ways to prepare an SD card, but Raspberry Pi recommends [Raspberry Pi Imager](https://www.raspberrypi.com/software/) on a Windows, Linux, or macOS computer with an SD card slot or SD card adapter.

Make sure to install the 64-bit version of Raspberry Pi OS.

The 8GB RAM Raspberry Pi 5 model is preferred for exploring an LLM.

## Collect the files into an archive

### Option 1: From the Docker container

By collecting the files you need to run in the container directly, you can transfer them in a compressed format. This means you can transfer all files with one command in the next section. Start by getting the model files in a separate folder. The bracketed file names are the absolute paths to the corresponding files that have been created in previous steps.

```bash
mkdir llama3-files
cd llama3-files
mv <cmake-out/examples/models/llama2/llama_main> .
mv <llama3/Meta-Llama-3-8B/> .
mv <llama3_kv_sdpa_xnn_qe_4_32.pte> .
```
Additionally, you need some additional library files. Locate the files and copy them as shown. They should be located in subdirectories of `cmake-out`, generated by ExecuTorch.
```bash
find ~ -name libllama_runner.so
find ~ -name libextension_module.so
```
```bash
mv <libllama_runner.so> .
mv <libextension_module.so> .
```

Compress the files into an archive by exiting the directory and running the `tar` command.
```bash
cd ..
tar -czvf llama3-files.tar.gz llama3-files
```

### Option 2: From the host machine terminal

This option may be slower due to the size of the files. Open a new terminal outside of the container shell to copy over the files. Replace `CONTAINER` with the name of the container, and the bracketed file names with the absolute paths to the corresponding files.

```bash
mkdir llama3-files
cd llama3-files
docker cp CONTAINER:<cmake-out/examples/models/llama2/llama_main> .
docker cp CONTAINER:<llama3/Meta-Llama-3-8B/> .
docker cp CONTAINER:<llama3_kv_sdpa_xnn_qe_4_32.pte> .
```
Additionally, you need some additional library files. Locate the files and copy them as shown. They should be located in subdirectories of `cmake-out`, generated by ExecuTorch.
```bash
find ~ -name libllama_runner.so
find ~ -name libextension_module.so
```
```bash
docker cp CONTAINER:<libllama_runner.so> .
docker cp CONTAINER:<libextension_module.so> .
```
If you don't know the container name, find it out by running the command.
```bash
docker container ls
```

Compress the files into an archive by exiting the directory and running the `tar` command.
```bash
cd ..
tar -czvf llama3-files.tar.gz llama3-files
```

## Transfer the archive
Now you can transfer the archive from the host machine or container to your Raspberry Pi 5. There are multiple ways to do this: via cloud storage services, with a USB thumb drive or using SSH. Use any method that is convenient for you. For example, you can use SCP running from a terminal in your Raspberry Pi 5 device as shown. Follow the same option as you did in the previous step.

### Option 1: From the Docker container

```bash
scp -P <port> llama3-files.tar.gz <user>@<host IP>:<destination>
```

### Option 2: From the host machine terminal
```bash
scp -P <port> <user>@<host IP>:</path/to/llama3-files.tar.gz>/* <destination>
```

where `<port>` is the port you set up, `<user>` the username and `<host IP>` the public IP address, all on your host machine. `</path/to/llama3-files.tar.gz>/*` is the path to the archive. The `<destination>` is where you want the file to end up on the Raspberry Pi 5. You may have to adapt the `scp` options to access the Raspberry Pi 5.

## Run the model
Finally, run the model in the terminal of your Raspberry Pi 5 using the same command as before.

```bash
llama_main --model_path=<model pte file> --tokenizer_path=<tokenizer.model> --cpu_threads=4 \
--prompt="Write a python script that prints the first 15 numbers in the Fibonacci series. Annotate the script with comments explaining what the code does."
```

From here, you can play around with different prompts on your embedded device.